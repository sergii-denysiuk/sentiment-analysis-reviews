{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB reviews's sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sergiidenysiuk/.virtualenvs/ml-team-wow/bin/python3.7\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Installation finished!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.executable)\n",
    "\n",
    "with open(\"requirements.txt\") as requirements_file:\n",
    "    requirements = [line.strip()\n",
    "                    for line in requirements_file.readlines()]\n",
    "\n",
    "for package in requirements:\n",
    "    !{sys.executable} -m pip -q install {package}\n",
    "print(\"Installation finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.py\n",
    "\n",
    "DATASET_FILE_PATH = \"aclImdb_v1.tar.gz\"\n",
    "DATASET_TEST_NEG_REVIEW = 'aclImdb/test/neg/*.txt'\n",
    "DATASET_TEST_POS_REVIEW = 'aclImdb/test/pos/*.txt'\n",
    "DATASET_TRAINING_NEG_REVIEW = 'aclImdb/train/neg/*.txt'\n",
    "DATASET_TRAINING_POS_REVIEW = 'aclImdb/train/pos/*.txt'\n",
    "DATASET_TRAINING_UNSUP_REVIEW = 'aclImdb/train/unsup/*.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The task\n",
    "\n",
    "Sentiment analysis is a challenging subject in machine learning. People express their emotions in language that is often obscured by sarcasm, ambiguity, and plays on words, all of which could be very misleading for both humans and computers. This is an example of sentiment analysis for movie review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data\n",
    "\n",
    "To achieve these goals, used a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. This dataset was collected in association with the following publication: http://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "import config\n",
    "\n",
    "tar = tarfile.open(config.DATASET_FILE_PATH, \"r:gz\")\n",
    "tar.extractall()\n",
    "tar.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing utils/read_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils/read_data.py\n",
    "import typing\n",
    "\n",
    "from parsers import base as base_parser\n",
    "\n",
    "\n",
    "def read_and_parse(path_pattern: str,\n",
    "                   parser: typing.Type[base_parser.BaseParser],\n",
    "                   **kwargs: typing.Any) -> Iterator[typing.Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Read and clean data from all files that match to given path-pattern.\n",
    "\n",
    "    :param path_pattern: pattern for files that must be processed\n",
    "    :param parser: parser class\n",
    "    :return: tuple with filename and parsed and cleaned data from file, example ('file', file_data)\n",
    "    \"\"\"\n",
    "    for filename in glob.glob(path_pattern):\n",
    "        with open(filename, 'r') as file:\n",
    "            yield filename, parser.parse(file.read(), **kwargs)\n",
    "\n",
    "\n",
    "def concat_sets(reviews: typing.Iterable[typing.Tuple[bool, typing.Iterable[typing.Tuple[str, str]]]],\n",
    "                negative_reviews: typing.Iterable[typing.Tuple[str, str]],\n",
    "                is_join: bool = False,\n",
    "                is_shuffle: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build dataset from given positive and negative datasets.\n",
    "\n",
    "    :param positive_reviews: list with positive data, example [(True, [('file_1', 'positive review text'), ...]), ...])\n",
    "    :param negative_reviews: list with negative data, example [('file_2', 'negative review text'), ...])\n",
    "    :param is_join: join items in each set from words to sentences\n",
    "    :param is_shuffle: shuffle positive and negative reviews\n",
    "    :return: table with filenames, reviews and it's sentiment values, respectively\n",
    "    \"\"\"\n",
    "    data: typing.List[typing.Tuple[str, typing.Union[list, str], str]] = []\n",
    "    dataset = ((positive_reviews, True), (negative_reviews, False))\n",
    "    \n",
    "    for reviews, sentiment in dataset:\n",
    "        data.extend(\n",
    "            (\n",
    "                filename,\n",
    "                \" \".join(filedata) if is_join else filedata,\n",
    "                sentiment,\n",
    "            ) for filename, filedata in reviews)\n",
    "\n",
    "    if is_shuffle:\n",
    "        random.shuffle(data)\n",
    "\n",
    "    return pd.DataFrame(data,\n",
    "                        columns=columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk.data\n",
    "\n",
    "try:\n",
    "    nltk.data.find('stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "EN_STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "class BaseParser(metaclass=ABCMeta):\n",
    "\n",
    "    @classmethod\n",
    "    def clean_html_markup(cls, text, parser='html.parser'):\n",
    "        \"\"\"Remove HTML markup.\"\"\"\n",
    "        return BeautifulSoup(text, parser).get_text()\n",
    "\n",
    "    @classmethod\n",
    "    def remove_non_letters(cls, text):\n",
    "        \"\"\"Remove non-letters.\"\"\"\n",
    "        return re.sub('[^a-zA-Z]', ' ', text)\n",
    "\n",
    "    @classmethod\n",
    "    def to_lower(cls, text):\n",
    "        \"\"\"Convert text to lowercase.\"\"\"\n",
    "        return text.lower()\n",
    "\n",
    "    @classmethod\n",
    "    def split_to_words(cls, text):\n",
    "        \"\"\"Split into individual words.\"\"\"\n",
    "        return text.split()\n",
    "\n",
    "    @classmethod\n",
    "    def remove_stopwords(cls, words, stopwords_list=EN_STOPWORDS):\n",
    "        \"\"\"Remove stopwords.\"\"\"\n",
    "        return [w for w in words if w not in EN_STOPWORDS]\n",
    "\n",
    "    @classmethod\n",
    "    def parse(cls, text, *args, **kwargs):\n",
    "        raise NotImplementedError(\"Should have implemented this\")\n",
    "\n",
    "\n",
    "class WordsParser(BaseParser):\n",
    "    \"\"\"\n",
    "    Implement processing raw HTML text\n",
    "    into segments of words for further learning.\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def parse(cls,\n",
    "              text,\n",
    "              is_remove_non_letters=True,\n",
    "              is_remove_stopwords=True):\n",
    "        \"\"\"\n",
    "        Get cleaned words from text.\n",
    "\n",
    "        :param text: text to parse\n",
    "        :type text: string\n",
    "        :param is_remove_non_letters: does non-letters have to be removed\n",
    "        :type is_remove_non_letters: bool\n",
    "        :param is_remove_stopwords: does stopwords have to be removed\n",
    "        :type is_remove_stopwords: bool\n",
    "        :return: list with cleaned words\n",
    "        :rtype: list\n",
    "        \"\"\"\n",
    "        result = cls.clean_html_markup(text)\n",
    "\n",
    "        if is_remove_non_letters:\n",
    "            result = cls.remove_non_letters(result)\n",
    "\n",
    "        result = cls.to_lower(result)\n",
    "        result = cls.split_to_words(result)\n",
    "\n",
    "        if is_remove_stopwords:\n",
    "            result = cls.remove_stopwords(result)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read, clean and parse train data...\n",
      "Done.\n",
      "Read, clean and parse test data...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Read, clean and parse train data...\")\n",
    "train_data = utils.concat_sets(\n",
    "    utils.read_and_parse(config.DATA_TRAINING_POS_REVIEW, word_parser.WordsParser),\n",
    "    utils.read_and_parse(config.DATA_TRAINING_NEG_REVIEW, word_parser.WordsParser),\n",
    "    columns=[\"id\", \"text\", \"sentiment\"],\n",
    "    is_join=True, is_shuffle=True)\n",
    "print(\"Done.\")\n",
    "\n",
    "print(\"Read, clean and parse test data...\")\n",
    "test_data = utils.concat_sets(\n",
    "    utils.read_and_parse(config.DATA_TEST_POS_REVIEW, word_parser.WordsParser),\n",
    "    utils.read_and_parse(config.DATA_TEST_NEG_REVIEW, word_parser.WordsParser),\n",
    "    columns=[\"id\", \"text\", \"sentiment\"],\n",
    "    is_join=True, is_shuffle=True)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aclImdb/train/neg/5925_3.txt</td>\n",
       "      <td>worse star trek tos episode maybe least gets v...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aclImdb/train/neg/1784_1.txt</td>\n",
       "      <td>ok taped tv missed start film seconds titles a...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aclImdb/train/neg/1682_2.txt</td>\n",
       "      <td>takashi shimizu great opportunity remake origi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aclImdb/train/neg/7339_3.txt</td>\n",
       "      <td>maya woman without interests dreams life away ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aclImdb/train/neg/4081_1.txt</td>\n",
       "      <td>worst film seen peter greenaway close dishonor...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             id  \\\n",
       "0  aclImdb/train/neg/5925_3.txt   \n",
       "1  aclImdb/train/neg/1784_1.txt   \n",
       "2  aclImdb/train/neg/1682_2.txt   \n",
       "3  aclImdb/train/neg/7339_3.txt   \n",
       "4  aclImdb/train/neg/4081_1.txt   \n",
       "\n",
       "                                                text  sentiment  \n",
       "0  worse star trek tos episode maybe least gets v...      False  \n",
       "1  ok taped tv missed start film seconds titles a...      False  \n",
       "2  takashi shimizu great opportunity remake origi...      False  \n",
       "3  maya woman without interests dreams life away ...      False  \n",
       "4  worst film seen peter greenaway close dishonor...      False  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aclImdb/test/pos/11149_8.txt</td>\n",
       "      <td>recently purchased universal marlene dietrich ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aclImdb/test/neg/9344_1.txt</td>\n",
       "      <td>want know writers movie consider funny robot c...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aclImdb/test/pos/10966_8.txt</td>\n",
       "      <td>ends declaration film seen improvisation makin...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aclImdb/test/neg/1672_1.txt</td>\n",
       "      <td>received movie pack called star movies cents m...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aclImdb/test/neg/5677_2.txt</td>\n",
       "      <td>show amazing fresh innovative idea first aired...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             id  \\\n",
       "0  aclImdb/test/pos/11149_8.txt   \n",
       "1   aclImdb/test/neg/9344_1.txt   \n",
       "2  aclImdb/test/pos/10966_8.txt   \n",
       "3   aclImdb/test/neg/1672_1.txt   \n",
       "4   aclImdb/test/neg/5677_2.txt   \n",
       "\n",
       "                                                text  sentiment  \n",
       "0  recently purchased universal marlene dietrich ...       True  \n",
       "1  want know writers movie consider funny robot c...      False  \n",
       "2  ends declaration film seen improvisation makin...       True  \n",
       "3  received movie pack called star movies cents m...      False  \n",
       "4  show amazing fresh innovative idea first aired...      False  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare text data (convert a collection of text documents to a matrix of token counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings\n",
    "\n",
    "The problem with text is that machine learning algorithms cannot work with raw text directly. The text must be converted into numbers, specifically, vectors of numbers.\n",
    "\n",
    "\n",
    "Bag of Words\n",
    "\n",
    "## Intro\n",
    "\n",
    "The **Bag of Words** (or **BoW**) model defines a texts' vocabulary, then models each text by counting the number of times each word appears. So it's just throws away all of the order information in the words and focuses on the occurrence of words in a document.\n",
    "\n",
    "For example, consider the following two sentences:\n",
    "* Sentence 1: \"The cat sat on the hat\"\n",
    "* Sentence 2: \"The dog ate the cat and the hat\"\n",
    "\n",
    "From these two sentences, vocabulary is as follows: *{ the, cat, sat, on, hat, dog, ate, and }*\n",
    "\n",
    "To get bags of words, count the number of times each word occurs in each sentence.\n",
    "* Sentence 1: { 2, 1, 1, 1, 1, 0, 0, 0 }\n",
    "* Sentence 2: { 3, 1, 0, 0, 1, 1, 1, 1 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the IMDB data, there is a very large number of reviews, which will give a large vocabulary. To limit the size of the feature vectors, choose some maximum vocabulary size. Below, used the $5000$ most frequent words (remember that stopwords have already been removed in previous step).\n",
    "\n",
    "**Note.** `CountVectorizer` comes with its own options to automatically do preprocessing, tokenization, and stop word removal for each of these, instead of specifying `None`, it's possible to use a built-in method or custom function, however, in this example, for data cleaning, custom parser is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the Bag Of Words...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating the Bag Of Words...\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=\"word\",\n",
    "                             max_features=5000)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learn a vocabulary from documents...\n",
      "Done.\n",
      "CPU times: user 3.04 s, sys: 91.3 ms, total: 3.13 s\n",
      "Wall time: 3.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Learn a vocabulary from documents...\")\n",
    "vectorizer.fit(train_data[\"text\"].tolist())\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abandoned',\n",
       " 'abc',\n",
       " 'abilities',\n",
       " 'ability',\n",
       " 'able',\n",
       " 'abraham',\n",
       " 'absence',\n",
       " 'absent',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'absurd',\n",
       " 'abuse',\n",
       " 'abusive',\n",
       " 'abysmal',\n",
       " 'academy',\n",
       " 'accent',\n",
       " 'accents',\n",
       " 'accept',\n",
       " 'acceptable',\n",
       " 'accepted',\n",
       " 'access',\n",
       " 'accident',\n",
       " 'accidentally',\n",
       " 'accompanied',\n",
       " 'accomplished',\n",
       " 'according',\n",
       " 'account',\n",
       " 'accuracy',\n",
       " 'accurate',\n",
       " 'accused',\n",
       " 'achieve',\n",
       " 'achieved',\n",
       " 'achievement',\n",
       " 'acid',\n",
       " 'across',\n",
       " 'act',\n",
       " 'acted',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'actions',\n",
       " 'activities',\n",
       " 'actor',\n",
       " 'actors',\n",
       " 'actress',\n",
       " 'actresses',\n",
       " 'acts',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'ad',\n",
       " 'adam',\n",
       " 'adams',\n",
       " 'adaptation',\n",
       " 'adaptations',\n",
       " 'adapted',\n",
       " 'add',\n",
       " 'added',\n",
       " 'adding',\n",
       " 'addition',\n",
       " 'adds',\n",
       " 'adequate',\n",
       " 'admire',\n",
       " 'admit',\n",
       " 'admittedly',\n",
       " 'adorable',\n",
       " 'adult',\n",
       " 'adults',\n",
       " 'advance',\n",
       " 'advanced',\n",
       " 'advantage',\n",
       " 'adventure',\n",
       " 'adventures',\n",
       " 'advertising',\n",
       " 'advice',\n",
       " 'advise',\n",
       " 'affair',\n",
       " 'affect',\n",
       " 'affected',\n",
       " 'afford',\n",
       " 'aforementioned',\n",
       " 'afraid',\n",
       " 'africa',\n",
       " 'african',\n",
       " 'afternoon',\n",
       " 'afterwards',\n",
       " 'age',\n",
       " 'aged',\n",
       " 'agent',\n",
       " 'agents',\n",
       " 'ages',\n",
       " 'aging',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'agreed',\n",
       " 'agrees',\n",
       " 'ah',\n",
       " 'ahead',\n",
       " 'aid',\n",
       " 'aids',\n",
       " 'aim',\n",
       " 'aimed',\n",
       " 'air',\n",
       " 'aired',\n",
       " 'airplane',\n",
       " 'airport',\n",
       " 'aka',\n",
       " 'akshay',\n",
       " 'al',\n",
       " 'alan',\n",
       " 'alas',\n",
       " 'albeit',\n",
       " 'albert',\n",
       " 'album',\n",
       " 'alcohol',\n",
       " 'alcoholic',\n",
       " 'alec',\n",
       " 'alert',\n",
       " 'alex',\n",
       " 'alexander',\n",
       " 'alfred',\n",
       " 'alice',\n",
       " 'alicia',\n",
       " 'alien',\n",
       " 'aliens',\n",
       " 'alike',\n",
       " 'alison',\n",
       " 'alive',\n",
       " 'allen',\n",
       " 'allow',\n",
       " 'allowed',\n",
       " 'allowing',\n",
       " 'allows',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'alongside',\n",
       " 'already',\n",
       " 'alright',\n",
       " 'also',\n",
       " 'alternate',\n",
       " 'alternative',\n",
       " 'although',\n",
       " 'altman',\n",
       " 'altogether',\n",
       " 'always',\n",
       " 'amanda',\n",
       " 'amateur',\n",
       " 'amateurish',\n",
       " 'amazed',\n",
       " 'amazing',\n",
       " 'amazingly',\n",
       " 'ambiguous',\n",
       " 'ambitious',\n",
       " 'america',\n",
       " 'american',\n",
       " 'americans',\n",
       " 'amitabh',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amount',\n",
       " 'amounts',\n",
       " 'amused',\n",
       " 'amusing',\n",
       " 'amy',\n",
       " 'analysis',\n",
       " 'ancient',\n",
       " 'anderson',\n",
       " 'andre',\n",
       " 'andrew',\n",
       " 'andrews',\n",
       " 'andy',\n",
       " 'angel',\n",
       " 'angela',\n",
       " 'angeles',\n",
       " 'angels',\n",
       " 'anger',\n",
       " 'angle',\n",
       " 'angles',\n",
       " 'angry',\n",
       " 'animal',\n",
       " 'animals',\n",
       " 'animated',\n",
       " 'animation',\n",
       " 'anime',\n",
       " 'ann',\n",
       " 'anna',\n",
       " 'anne',\n",
       " 'annie',\n",
       " 'annoyed',\n",
       " 'annoying',\n",
       " 'another',\n",
       " 'answer',\n",
       " 'answers',\n",
       " 'anthony',\n",
       " 'anti',\n",
       " 'antics',\n",
       " 'antonioni',\n",
       " 'antwone',\n",
       " 'anybody',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anyways',\n",
       " 'anywhere',\n",
       " 'apart',\n",
       " 'apartment',\n",
       " 'ape',\n",
       " 'apes',\n",
       " 'appalling',\n",
       " 'apparent',\n",
       " 'apparently',\n",
       " 'appeal',\n",
       " 'appealing',\n",
       " 'appear',\n",
       " 'appearance',\n",
       " 'appearances',\n",
       " 'appeared',\n",
       " 'appearing',\n",
       " 'appears',\n",
       " 'appreciate',\n",
       " 'appreciated',\n",
       " 'appreciation',\n",
       " 'approach',\n",
       " 'appropriate',\n",
       " 'april',\n",
       " 'area',\n",
       " 'areas',\n",
       " 'arguably',\n",
       " 'argue',\n",
       " 'argument',\n",
       " 'arm',\n",
       " 'armed',\n",
       " 'arms',\n",
       " 'army',\n",
       " 'arnold',\n",
       " 'around',\n",
       " 'arrested',\n",
       " 'arrival',\n",
       " 'arrive',\n",
       " 'arrived',\n",
       " 'arrives',\n",
       " 'arrogant',\n",
       " 'art',\n",
       " 'arthur',\n",
       " 'artificial',\n",
       " 'artist',\n",
       " 'artistic',\n",
       " 'artists',\n",
       " 'arts',\n",
       " 'ashamed',\n",
       " 'ashley',\n",
       " 'asian',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'asks',\n",
       " 'asleep',\n",
       " 'aspect',\n",
       " 'aspects',\n",
       " 'ass',\n",
       " 'assassin',\n",
       " 'assault',\n",
       " 'assigned',\n",
       " 'assistant',\n",
       " 'associated',\n",
       " 'assume',\n",
       " 'assumed',\n",
       " 'astaire',\n",
       " 'astonishing',\n",
       " 'atlantis',\n",
       " 'atmosphere',\n",
       " 'atmospheric',\n",
       " 'atrocious',\n",
       " 'attached',\n",
       " 'attack',\n",
       " 'attacked',\n",
       " 'attacks',\n",
       " 'attempt',\n",
       " 'attempted',\n",
       " 'attempting',\n",
       " 'attempts',\n",
       " 'attend',\n",
       " 'attention',\n",
       " 'attitude',\n",
       " 'attitudes',\n",
       " 'attorney',\n",
       " 'attracted',\n",
       " 'attraction',\n",
       " 'attractive',\n",
       " 'audience',\n",
       " 'audiences',\n",
       " 'audio',\n",
       " 'aunt',\n",
       " 'austen',\n",
       " 'austin',\n",
       " 'australia',\n",
       " 'australian',\n",
       " 'authentic',\n",
       " 'author',\n",
       " 'authority',\n",
       " 'available',\n",
       " 'average',\n",
       " 'avoid',\n",
       " 'avoided',\n",
       " 'awake',\n",
       " 'award',\n",
       " 'awards',\n",
       " 'aware',\n",
       " 'away',\n",
       " 'awe',\n",
       " 'awesome',\n",
       " 'awful',\n",
       " 'awfully',\n",
       " 'awhile',\n",
       " 'awkward',\n",
       " 'babe',\n",
       " 'baby',\n",
       " 'bacall',\n",
       " 'back',\n",
       " 'backdrop',\n",
       " 'background',\n",
       " 'backgrounds',\n",
       " 'bad',\n",
       " 'badly',\n",
       " 'bag',\n",
       " 'baker',\n",
       " 'bakshi',\n",
       " 'balance',\n",
       " 'baldwin',\n",
       " 'ball',\n",
       " 'ballet',\n",
       " 'balls',\n",
       " 'band',\n",
       " 'bands',\n",
       " 'bang',\n",
       " 'bank',\n",
       " 'banned',\n",
       " 'bar',\n",
       " 'barbara',\n",
       " 'bare',\n",
       " 'barely',\n",
       " 'bargain',\n",
       " 'barry',\n",
       " 'barrymore',\n",
       " 'base',\n",
       " 'baseball',\n",
       " 'based',\n",
       " 'basement',\n",
       " 'basic',\n",
       " 'basically',\n",
       " 'basis',\n",
       " 'basketball',\n",
       " 'bat',\n",
       " 'bath',\n",
       " 'bathroom',\n",
       " 'batman',\n",
       " 'battle',\n",
       " 'battles',\n",
       " 'bay',\n",
       " 'bbc',\n",
       " 'beach',\n",
       " 'bear',\n",
       " 'bears',\n",
       " 'beast',\n",
       " 'beat',\n",
       " 'beaten',\n",
       " 'beating',\n",
       " 'beats',\n",
       " 'beatty',\n",
       " 'beautiful',\n",
       " 'beautifully',\n",
       " 'beauty',\n",
       " 'became',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'bed',\n",
       " 'bedroom',\n",
       " 'beer',\n",
       " 'began',\n",
       " 'begin',\n",
       " 'beginning',\n",
       " 'begins',\n",
       " 'behave',\n",
       " 'behavior',\n",
       " 'behind',\n",
       " 'beings',\n",
       " 'bela',\n",
       " 'belief',\n",
       " 'beliefs',\n",
       " 'believable',\n",
       " 'believe',\n",
       " 'believed',\n",
       " 'believes',\n",
       " 'believing',\n",
       " 'bell',\n",
       " 'belong',\n",
       " 'belongs',\n",
       " 'beloved',\n",
       " 'belushi',\n",
       " 'ben',\n",
       " 'beneath',\n",
       " 'benefit',\n",
       " 'bergman',\n",
       " 'berlin',\n",
       " 'besides',\n",
       " 'best',\n",
       " 'bet',\n",
       " 'bette',\n",
       " 'better',\n",
       " 'bettie',\n",
       " 'betty',\n",
       " 'beyond',\n",
       " 'bible',\n",
       " 'big',\n",
       " 'bigger',\n",
       " 'biggest',\n",
       " 'biko',\n",
       " 'bill',\n",
       " 'billed',\n",
       " 'billy',\n",
       " 'bin',\n",
       " 'biography',\n",
       " 'bird',\n",
       " 'birds',\n",
       " 'birth',\n",
       " 'birthday',\n",
       " 'bit',\n",
       " 'bite',\n",
       " 'bits',\n",
       " 'bitter',\n",
       " 'bizarre',\n",
       " 'black',\n",
       " 'blade',\n",
       " 'blah',\n",
       " 'blair',\n",
       " 'blake',\n",
       " 'blame',\n",
       " 'bland',\n",
       " 'blank',\n",
       " 'blast',\n",
       " 'blatant',\n",
       " 'bleak',\n",
       " 'blend',\n",
       " 'blew',\n",
       " 'blind',\n",
       " 'blob',\n",
       " 'block',\n",
       " 'blockbuster',\n",
       " 'blond',\n",
       " 'blonde',\n",
       " 'blood',\n",
       " 'bloody',\n",
       " 'blow',\n",
       " 'blowing',\n",
       " 'blown',\n",
       " 'blows',\n",
       " 'blue',\n",
       " 'blues',\n",
       " 'blunt',\n",
       " 'bo',\n",
       " 'board',\n",
       " 'boat',\n",
       " 'bob',\n",
       " 'bobby',\n",
       " 'bodies',\n",
       " 'body',\n",
       " 'bold',\n",
       " 'boll',\n",
       " 'bollywood',\n",
       " 'bomb',\n",
       " 'bond',\n",
       " 'bone',\n",
       " 'bonus',\n",
       " 'book',\n",
       " 'books',\n",
       " 'boom',\n",
       " 'boot',\n",
       " 'border',\n",
       " 'bore',\n",
       " 'bored',\n",
       " 'boredom',\n",
       " 'boring',\n",
       " 'born',\n",
       " 'borrowed',\n",
       " 'boss',\n",
       " 'bother',\n",
       " 'bothered',\n",
       " 'bottle',\n",
       " 'bottom',\n",
       " 'bought',\n",
       " 'bound',\n",
       " 'bourne',\n",
       " 'box',\n",
       " 'boxing',\n",
       " 'boy',\n",
       " 'boyfriend',\n",
       " 'boyle',\n",
       " 'boys',\n",
       " 'brad',\n",
       " 'brady',\n",
       " 'brain',\n",
       " 'brains',\n",
       " 'branagh',\n",
       " 'brand',\n",
       " 'brando',\n",
       " 'brave',\n",
       " 'brazil',\n",
       " 'break',\n",
       " 'breaking',\n",
       " 'breaks',\n",
       " 'breasts',\n",
       " 'breath',\n",
       " 'breathtaking',\n",
       " 'brenda',\n",
       " 'brian',\n",
       " 'bride',\n",
       " 'bridge',\n",
       " 'brief',\n",
       " 'briefly',\n",
       " 'bright',\n",
       " 'brilliance',\n",
       " 'brilliant',\n",
       " 'brilliantly',\n",
       " 'bring',\n",
       " 'bringing',\n",
       " 'brings',\n",
       " 'britain',\n",
       " 'british',\n",
       " 'broad',\n",
       " 'broadcast',\n",
       " 'broadway',\n",
       " 'broke',\n",
       " 'broken',\n",
       " 'brooklyn',\n",
       " 'brooks',\n",
       " 'brosnan',\n",
       " 'brother',\n",
       " 'brothers',\n",
       " 'brought',\n",
       " 'brown',\n",
       " 'bruce',\n",
       " 'brutal',\n",
       " 'brutality',\n",
       " 'brutally',\n",
       " 'buck',\n",
       " 'bucks',\n",
       " 'bud',\n",
       " 'buddies',\n",
       " 'buddy',\n",
       " 'budget',\n",
       " 'buff',\n",
       " 'buffalo',\n",
       " 'buffs',\n",
       " 'bug',\n",
       " 'bugs',\n",
       " 'build',\n",
       " 'building',\n",
       " 'buildings',\n",
       " 'builds',\n",
       " 'built',\n",
       " 'bull',\n",
       " 'bullet',\n",
       " 'bullets',\n",
       " 'bumbling',\n",
       " 'bunch',\n",
       " 'bunny',\n",
       " 'buried',\n",
       " 'burn',\n",
       " 'burned',\n",
       " 'burning',\n",
       " 'burns',\n",
       " 'burt',\n",
       " 'burton',\n",
       " 'bus',\n",
       " 'bush',\n",
       " 'business',\n",
       " 'businessman',\n",
       " 'buster',\n",
       " 'busy',\n",
       " 'butler',\n",
       " 'butt',\n",
       " 'button',\n",
       " 'buy',\n",
       " 'buying',\n",
       " 'cabin',\n",
       " 'cable',\n",
       " 'cage',\n",
       " 'cagney',\n",
       " 'caine',\n",
       " 'cake',\n",
       " 'caliber',\n",
       " 'california',\n",
       " 'call',\n",
       " 'called',\n",
       " 'calling',\n",
       " 'calls',\n",
       " 'calm',\n",
       " 'came',\n",
       " 'cameo',\n",
       " 'cameos',\n",
       " 'camera',\n",
       " 'cameras',\n",
       " 'cameron',\n",
       " 'camp',\n",
       " 'campbell',\n",
       " 'campy',\n",
       " 'canada',\n",
       " 'canadian',\n",
       " 'candy',\n",
       " 'cannibal',\n",
       " 'cannot',\n",
       " 'cant',\n",
       " 'capable',\n",
       " 'capital',\n",
       " 'captain',\n",
       " 'captivating',\n",
       " 'capture',\n",
       " 'captured',\n",
       " 'captures',\n",
       " 'capturing',\n",
       " 'car',\n",
       " 'card',\n",
       " 'cardboard',\n",
       " 'cards',\n",
       " 'care',\n",
       " 'cared',\n",
       " 'career',\n",
       " 'careers',\n",
       " 'careful',\n",
       " 'carefully',\n",
       " 'carell',\n",
       " 'cares',\n",
       " 'caring',\n",
       " 'carl',\n",
       " 'carla',\n",
       " 'carol',\n",
       " 'carpenter',\n",
       " 'carradine',\n",
       " 'carrey',\n",
       " 'carrie',\n",
       " 'carried',\n",
       " 'carries',\n",
       " 'carry',\n",
       " 'carrying',\n",
       " 'cars',\n",
       " 'carter',\n",
       " 'cartoon',\n",
       " 'cartoons',\n",
       " 'cary',\n",
       " 'case',\n",
       " 'cases',\n",
       " 'cash',\n",
       " 'cassidy',\n",
       " 'cast',\n",
       " 'casting',\n",
       " 'castle',\n",
       " 'cat',\n",
       " 'catch',\n",
       " 'catches',\n",
       " 'catching',\n",
       " 'catchy',\n",
       " 'category',\n",
       " 'catherine',\n",
       " 'catholic',\n",
       " 'cats',\n",
       " 'caught',\n",
       " 'cause',\n",
       " 'caused',\n",
       " 'causes',\n",
       " 'causing',\n",
       " 'cave',\n",
       " 'cd',\n",
       " 'celebrity',\n",
       " 'cell',\n",
       " 'celluloid',\n",
       " 'center',\n",
       " 'centered',\n",
       " 'centers',\n",
       " 'central',\n",
       " 'century',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'cg',\n",
       " 'cgi',\n",
       " 'chain',\n",
       " 'chair',\n",
       " 'challenge',\n",
       " 'challenging',\n",
       " 'championship',\n",
       " 'chan',\n",
       " 'chance',\n",
       " 'chances',\n",
       " 'change',\n",
       " 'changed',\n",
       " 'changes',\n",
       " 'changing',\n",
       " 'channel',\n",
       " 'channels',\n",
       " 'chaos',\n",
       " 'chaplin',\n",
       " 'chapter',\n",
       " 'character',\n",
       " 'characterization',\n",
       " 'characters',\n",
       " 'charge',\n",
       " 'charisma',\n",
       " 'charismatic',\n",
       " 'charles',\n",
       " 'charlie',\n",
       " 'charlotte',\n",
       " 'charm',\n",
       " 'charming',\n",
       " 'chase',\n",
       " 'chased',\n",
       " 'chases',\n",
       " 'chasing',\n",
       " 'che',\n",
       " 'cheap',\n",
       " 'cheated',\n",
       " 'cheating',\n",
       " 'check',\n",
       " 'checked',\n",
       " 'checking',\n",
       " 'cheek',\n",
       " 'cheese',\n",
       " 'cheesy',\n",
       " 'chemistry',\n",
       " 'chess',\n",
       " 'chest',\n",
       " 'chicago',\n",
       " 'chick',\n",
       " 'chicken',\n",
       " 'chicks',\n",
       " 'chief',\n",
       " 'child',\n",
       " 'childhood',\n",
       " 'childish',\n",
       " 'children',\n",
       " 'chilling',\n",
       " 'china',\n",
       " 'chinese',\n",
       " 'choice',\n",
       " 'choices',\n",
       " 'choose',\n",
       " 'chooses',\n",
       " 'choreographed',\n",
       " 'choreography',\n",
       " 'chorus',\n",
       " 'chose',\n",
       " 'chosen',\n",
       " 'chris',\n",
       " 'christ',\n",
       " 'christian',\n",
       " 'christianity',\n",
       " 'christians',\n",
       " 'christmas',\n",
       " 'christopher',\n",
       " 'christy',\n",
       " 'chuck',\n",
       " 'church',\n",
       " 'cia',\n",
       " 'cinderella',\n",
       " 'cinema',\n",
       " 'cinematic',\n",
       " 'cinematographer',\n",
       " 'cinematography',\n",
       " 'circle',\n",
       " 'circumstances',\n",
       " 'cities',\n",
       " 'citizen',\n",
       " 'city',\n",
       " 'civil',\n",
       " 'civilization',\n",
       " 'claim',\n",
       " 'claimed',\n",
       " 'claims',\n",
       " 'claire',\n",
       " 'clark',\n",
       " 'class',\n",
       " 'classes',\n",
       " 'classic',\n",
       " 'classical',\n",
       " 'classics',\n",
       " 'claus',\n",
       " 'clean',\n",
       " 'clear',\n",
       " 'clearly',\n",
       " 'clever',\n",
       " 'cleverly',\n",
       " 'clich',\n",
       " 'cliche',\n",
       " 'cliff',\n",
       " 'climactic',\n",
       " 'climax',\n",
       " 'clint',\n",
       " 'clip',\n",
       " 'clips',\n",
       " 'clock',\n",
       " 'close',\n",
       " 'closed',\n",
       " 'closely',\n",
       " 'closer',\n",
       " 'closest',\n",
       " 'closet',\n",
       " 'closing',\n",
       " 'clothes',\n",
       " 'clothing',\n",
       " 'clown',\n",
       " 'club',\n",
       " 'clue',\n",
       " 'clues',\n",
       " 'clumsy',\n",
       " 'co',\n",
       " 'coach',\n",
       " 'coast',\n",
       " 'code',\n",
       " 'coffee',\n",
       " 'coherent',\n",
       " 'cold',\n",
       " 'cole',\n",
       " 'collection',\n",
       " 'college',\n",
       " 'colonel',\n",
       " 'color',\n",
       " 'colorful',\n",
       " 'colors',\n",
       " 'colour',\n",
       " 'columbo',\n",
       " 'com',\n",
       " 'combat',\n",
       " 'combination',\n",
       " 'combine',\n",
       " 'combined',\n",
       " 'come',\n",
       " 'comedian',\n",
       " 'comedic',\n",
       " 'comedies',\n",
       " 'comedy',\n",
       " 'comes',\n",
       " 'comfort',\n",
       " 'comfortable',\n",
       " 'comic',\n",
       " 'comical',\n",
       " 'comics',\n",
       " 'coming',\n",
       " 'command',\n",
       " 'comment',\n",
       " 'commentary',\n",
       " 'commented',\n",
       " 'comments',\n",
       " 'commercial',\n",
       " 'commercials',\n",
       " 'commit',\n",
       " 'committed',\n",
       " 'common',\n",
       " 'communist',\n",
       " 'community',\n",
       " 'companies',\n",
       " 'companion',\n",
       " 'company',\n",
       " 'compare',\n",
       " 'compared',\n",
       " 'comparing',\n",
       " 'comparison',\n",
       " 'compassion',\n",
       " 'compelled',\n",
       " 'compelling',\n",
       " 'competent',\n",
       " 'competition',\n",
       " 'complain',\n",
       " 'complaint',\n",
       " 'complete',\n",
       " 'completely',\n",
       " 'complex',\n",
       " 'complexity',\n",
       " 'complicated',\n",
       " 'composed',\n",
       " 'composer',\n",
       " 'computer',\n",
       " 'con',\n",
       " 'conceived',\n",
       " 'concept',\n",
       " 'concern',\n",
       " 'concerned',\n",
       " 'concerning',\n",
       " 'concerns',\n",
       " 'concert',\n",
       " 'conclusion',\n",
       " 'condition',\n",
       " 'conditions',\n",
       " 'confess',\n",
       " 'confidence',\n",
       " 'conflict',\n",
       " 'conflicts',\n",
       " 'confrontation',\n",
       " 'confused',\n",
       " 'confusing',\n",
       " 'confusion',\n",
       " 'connect',\n",
       " 'connected',\n",
       " 'connection',\n",
       " 'connery',\n",
       " 'conscious',\n",
       " 'consequences',\n",
       " 'conservative',\n",
       " 'consider',\n",
       " 'considerable',\n",
       " 'considered',\n",
       " 'considering',\n",
       " 'consistent',\n",
       " 'consistently',\n",
       " 'consists',\n",
       " 'conspiracy',\n",
       " 'constant',\n",
       " 'constantly',\n",
       " 'constructed',\n",
       " 'construction',\n",
       " 'contact',\n",
       " 'contain',\n",
       " 'contained',\n",
       " 'contains',\n",
       " 'contemporary',\n",
       " 'content',\n",
       " 'contest',\n",
       " 'context',\n",
       " 'continue',\n",
       " 'continued',\n",
       " 'continues',\n",
       " 'continuity',\n",
       " 'contract',\n",
       " 'contrary',\n",
       " 'contrast',\n",
       " 'contrived',\n",
       " 'control',\n",
       " 'controversial',\n",
       " 'conventional',\n",
       " 'conversation',\n",
       " 'conversations',\n",
       " 'convey',\n",
       " 'convince',\n",
       " 'convinced',\n",
       " 'convincing',\n",
       " 'convincingly',\n",
       " 'convoluted',\n",
       " 'cook',\n",
       " 'cool',\n",
       " 'cooper',\n",
       " 'cop',\n",
       " 'copies',\n",
       " 'cops',\n",
       " 'copy',\n",
       " 'core',\n",
       " 'corner',\n",
       " 'corny',\n",
       " 'corporate',\n",
       " 'corpse',\n",
       " 'correct',\n",
       " 'correctly',\n",
       " 'corrupt',\n",
       " 'corruption',\n",
       " 'cost',\n",
       " 'costs',\n",
       " 'costume',\n",
       " 'costumes',\n",
       " 'could',\n",
       " 'count',\n",
       " 'counter',\n",
       " 'countless',\n",
       " 'countries',\n",
       " 'country',\n",
       " 'countryside',\n",
       " 'couple',\n",
       " 'couples',\n",
       " 'courage',\n",
       " 'course',\n",
       " 'court',\n",
       " 'cousin',\n",
       " 'cover',\n",
       " 'covered',\n",
       " 'covers',\n",
       " 'cowboy',\n",
       " 'cox',\n",
       " 'crack',\n",
       " 'cracking',\n",
       " 'craft',\n",
       " 'crafted',\n",
       " 'craig',\n",
       " 'crap',\n",
       " 'crappy',\n",
       " 'crash',\n",
       " 'craven',\n",
       " 'crawford',\n",
       " 'crazed',\n",
       " 'crazy',\n",
       " 'cream',\n",
       " 'create',\n",
       " 'created',\n",
       " 'creates',\n",
       " ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-4910390768c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "vectorizer.get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encode each train movie review document to vector...\n",
      "Done.\n",
      "Encode each test movie review document to vector...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Encode each train movie review document to vector...\")\n",
    "train_vectors = vectorizer.transform(train_data[\"text\"].tolist()).toarray()\n",
    "print(\"Done.\")\n",
    "\n",
    "print(\"Encode each test movie review document to vector...\")\n",
    "test_vectors = vectorizer.transform(test_data[\"text\"].tolist()).toarray()\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_vectors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-bf322cdf2c1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_vectors' is not defined"
     ]
    }
   ],
   "source": [
    "train_vectors\n",
    "train_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load classifiers/sklearn.py\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "def random_forest(train_review_bag_of_words, train_review_sentiments,\n",
    "                  n_estimators=100):\n",
    "    \"\"\"Random Forest classifier.\"\"\"\n",
    "    forest = RandomForestClassifier(n_estimators=n_estimators)\n",
    "    forest = forest.fit(train_review_bag_of_words, train_review_sentiments)\n",
    "    return forest\n",
    "\n",
    "\n",
    "def naive_bayes_gaussian(train_review_bag_of_words, train_review_sentiments):\n",
    "    \"\"\"Naive Bayes Gaussian classifier.\"\"\"\n",
    "    nbg = GaussianNB()\n",
    "    nbg = nbg.fit(train_review_bag_of_words, train_review_sentiments)\n",
    "    return nbg\n",
    "\n",
    "\n",
    "def naive_bayes_multinomial(train_review_bag_of_words, train_review_sentiments):\n",
    "    \"\"\"Naive Bayes Multinomial classifier.\"\"\"\n",
    "    nbm = MultinomialNB()\n",
    "    nbm = nbm.fit(train_review_bag_of_words, train_review_sentiments)\n",
    "    return nbm\n",
    "\n",
    "\n",
    "def naive_bayes_bernoulli(train_review_bag_of_words, train_review_sentiments):\n",
    "    \"\"\"Naive Bayes Bernoulli classifier.\"\"\"\n",
    "    nbb = BernoulliNB()\n",
    "    nbb = nbb.fit(train_review_bag_of_words, train_review_sentiments)\n",
    "    return nbb\n",
    "\n",
    "\n",
    "def k_nearest_neighbors(train_review_bag_of_words, train_review_sentiments,\n",
    "                        n_neighbors=100, weights='uniform', algorithm='auto'):\n",
    "    \"\"\"k-Nnearest Neighbors classifier.\"\"\"\n",
    "    knn = KNeighborsClassifier(\n",
    "        n_neighbors=n_neighbors, weights=weights, algorithm=algorithm)\n",
    "    knn = knn.fit(train_review_bag_of_words, train_review_sentiments)\n",
    "    return knn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Random Forest...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Training the Random Forest...\")\n",
    "n_estimators = 100\n",
    "test_sentiments_predicted_rf = sk_classifiers.random_forest(\n",
    "    train_vectors, train_data[\"sentiment\"], n_estimators=n_estimators).predict(test_vectors)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Naive Bayes Gaussian...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Training the Naive Bayes Gaussian...\")\n",
    "test_sentiments_predicted_nbg = sk_classifiers.naive_bayes_gaussian(\n",
    "    train_vectors, train_data[\"sentiment\"]).predict(test_vectors)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Naive Bayes Multinomial...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Training the Naive Bayes Multinomial...\")\n",
    "test_sentiments_predicted_nbm = sk_classifiers.naive_bayes_multinomial(\n",
    "    train_vectors, train_data[\"sentiment\"]).predict(test_vectors)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Naive Bayes Bernoulli...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Training the Naive Bayes Bernoulli...\")\n",
    "test_sentiments_predicted_nbb = sk_classifiers.naive_bayes_bernoulli(\n",
    "    train_vectors, train_data[\"sentiment\"]).predict(test_vectors)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the k-Nearest Neighbors...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Training the k-Nearest Neighbors...\")\n",
    "n_neighbors = 100\n",
    "test_sentiments_predicted_knn = sk_classifiers.k_nearest_neighbors(\n",
    "    train_vectors, train_data[\"sentiment\"], n_neighbors=n_neighbors).predict(test_vectors)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check models' accuracy and save summary to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s write_results_to_csv utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write Random Forest results to bag-of-words-sklearn-rf-model.csv\n",
      "Done.\n",
      "Write Naive Bayes Gaussian results to bag-of-words-sklearn-nbg-model.csv\n",
      "Done.\n",
      "Write Naive Bayes Multinomial results to bag-of-words-sklearn-nbm-model.csv\n",
      "Done.\n",
      "Write Naive Bayes Bernoulli results to bag-of-words-sklearn-nbb-model.csv\n",
      "Done.\n",
      "Write k-Nearest Neighbors results to bag-of-words-sklearn-knn-model.csv\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "filename_sklearn_rf = 'bag-of-words-sklearn-rf-model.csv'\n",
    "filename_sklearn_nbg = 'bag-of-words-sklearn-nbg-model.csv'\n",
    "filename_sklearn_nbm = 'bag-of-words-sklearn-nbm-model.csv'\n",
    "filename_sklearn_nbb = 'bag-of-words-sklearn-nbb-model.csv'\n",
    "filename_sklearn_knn = 'bag-of-words-sklearn-knn-model.csv'\n",
    "filename_summary = 'bag-of-words-summary.txt'\n",
    "\n",
    "print(f\"Write Random Forest results to {filename_sklearn_rf}\")\n",
    "utils.write_results_to_csv(\n",
    "    test_data[\"id\"],\n",
    "    test_data[\"sentiment\"],\n",
    "    test_sentiments_predicted_rf,\n",
    "    filename_sklearn_rf)\n",
    "print(\"Done.\")\n",
    "\n",
    "print(f\"Write Naive Bayes Gaussian results to {filename_sklearn_nbg}\")\n",
    "utils.write_results_to_csv(\n",
    "    test_data[\"id\"],\n",
    "    test_data[\"sentiment\"],\n",
    "    test_sentiments_predicted_nbg,\n",
    "    filename_sklearn_nbg)\n",
    "print(\"Done.\")\n",
    "\n",
    "print(f\"Write Naive Bayes Multinomial results to {filename_sklearn_nbm}\")\n",
    "utils.write_results_to_csv(\n",
    "    test_data[\"id\"],\n",
    "    test_data[\"sentiment\"],\n",
    "    test_sentiments_predicted_nbm,\n",
    "    filename_sklearn_nbm)\n",
    "print(\"Done.\")\n",
    "\n",
    "print(f\"Write Naive Bayes Bernoulli results to {filename_sklearn_nbb}\")\n",
    "utils.write_results_to_csv(\n",
    "    test_data[\"id\"],\n",
    "    test_data[\"sentiment\"],\n",
    "    test_sentiments_predicted_nbb,\n",
    "    filename_sklearn_nbb)\n",
    "print(\"Done.\")\n",
    "\n",
    "print(f\"Write k-Nearest Neighbors results to {filename_sklearn_knn}\")\n",
    "utils.write_results_to_csv(\n",
    "    test_data[\"id\"],\n",
    "    test_data[\"sentiment\"],\n",
    "    test_sentiments_predicted_knn,\n",
    "    filename_sklearn_knn)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s calculate_accuracy utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s count_words utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write summary results to bag-of-words-summary.txt\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Write summary results to {filename_summary}\")\n",
    "with open(filename_summary, \"w\") as file_summary:\n",
    "    print('Size of train dataset: {size}'.format(\n",
    "        size=len(train_data[\"id\"])), file=file_summary)\n",
    "\n",
    "    print('Size of test dataset: {size}'.format(\n",
    "        size=len(test_data[\"id\"])), file=file_summary)\n",
    "\n",
    "    print('', file=file_summary)\n",
    "\n",
    "    print('Number of trees in Random Forest: {trees}'.format(\n",
    "        trees=n_estimators), file=file_summary)\n",
    "\n",
    "    print('Number of neighbors in KNN: {neighbors}'.format(\n",
    "        neighbors=n_neighbors), file=file_summary)\n",
    "\n",
    "    print('', file=file_summary)\n",
    "\n",
    "    print('Accuracy of the the Random Forest sklearn: {accuracy}'.format(\n",
    "        accuracy=utils.calculate_accuracy(\n",
    "            test_data[\"sentiment\"], test_sentiments_predicted_rf)), file=file_summary)\n",
    "\n",
    "    print('Accuracy of the Naive Bayes Gaussian sklearn: {accuracy}'.format(\n",
    "        accuracy=utils.calculate_accuracy(\n",
    "            test_data[\"sentiment\"], test_sentiments_predicted_nbg)), file=file_summary)\n",
    "\n",
    "    print('Accuracy of the Naive Bayes Multinomial sklearn: {accuracy}'.format(\n",
    "        accuracy=utils.calculate_accuracy(\n",
    "            test_data[\"sentiment\"], test_sentiments_predicted_nbm)), file=file_summary)\n",
    "\n",
    "    print('Accuracy of the Naive Bayes Bernoulli sklearn: {accuracy}'.format(\n",
    "        accuracy=utils.calculate_accuracy(\n",
    "            test_data[\"sentiment\"], test_sentiments_predicted_nbb)), file=file_summary)\n",
    "\n",
    "    print('Accuracy of the k-Nearest Neighbors sklearn: {accuracy}'.format(\n",
    "        accuracy=utils.calculate_accuracy(\n",
    "            test_data[\"sentiment\"], test_sentiments_predicted_knn)), file=file_summary)\n",
    "\n",
    "    print('', file=file_summary)\n",
    "\n",
    "    print('Count of each word in train dataset: {counts}'.format(\n",
    "        counts=utils.count_words(vectorizer.get_feature_names(), train_data[\"text\"])), file=file_summary)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
